{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "F-Y-S4aeSBqt"
      },
      "outputs": [],
      "source": [
        "# Importing Important Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing and Customizing the datase\n",
        "df = pd.read_csv('Heart_Disease.csv')\n",
        "df = df.drop_duplicates()\n",
        "df = df.dropna()\n",
        "df = df.interpolate()\n",
        "print(df.head())\n",
        "df = df[['age', 'cigsPerDay', 'totChol', 'sysBP', 'diaBP', 'TenYearCHD']]\n",
        "print(df.head())\n",
        "\n",
        "# All continuous features\n",
        "x1 = np.array(df['age'])\n",
        "x2 = np.array(df['cigsPerDay'])\n",
        "x3 = np.array(df['totChol'])\n",
        "x4 = np.array(df['sysBP'])\n",
        "x5 = np.array(df['diaBP'])\n",
        "y = np.array(df['TenYearCHD'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBp45dakTHbk",
        "outputId": "ad27330c-0469-4c4f-8f41-f826b1a12ce5"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n",
            "0     1   39        4.0              0         0.0     0.0                0   \n",
            "1     0   46        2.0              0         0.0     0.0                0   \n",
            "2     1   48        1.0              1        20.0     0.0                0   \n",
            "3     0   61        3.0              1        30.0     0.0                0   \n",
            "4     0   46        3.0              1        23.0     0.0                0   \n",
            "\n",
            "   prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \\\n",
            "0             0         0    195.0  106.0   70.0  26.97       80.0     77.0   \n",
            "1             0         0    250.0  121.0   81.0  28.73       95.0     76.0   \n",
            "2             0         0    245.0  127.5   80.0  25.34       75.0     70.0   \n",
            "3             1         0    225.0  150.0   95.0  28.58       65.0    103.0   \n",
            "4             0         0    285.0  130.0   84.0  23.10       85.0     85.0   \n",
            "\n",
            "   TenYearCHD  \n",
            "0           0  \n",
            "1           0  \n",
            "2           0  \n",
            "3           1  \n",
            "4           0  \n",
            "   age  cigsPerDay  totChol  sysBP  diaBP  TenYearCHD\n",
            "0   39         0.0    195.0  106.0   70.0           0\n",
            "1   46         0.0    250.0  121.0   81.0           0\n",
            "2   48        20.0    245.0  127.5   80.0           0\n",
            "3   61        30.0    225.0  150.0   95.0           1\n",
            "4   46        23.0    285.0  130.0   84.0           0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Performaing train test split\n",
        "x1, x1_t, \\\n",
        "x2, x2_t, \\\n",
        "x3, x3_t, \\\n",
        "x4, x4_t, \\\n",
        "x5, x5_t, \\\n",
        "y_train, y_test = train_test_split(\n",
        "    x1, x2, x3, x4, x5, y,\n",
        "    test_size = 0.2,\n",
        "    random_state = 42\n",
        ")\n",
        "\n",
        "# Extracting minimum and maximum values of all\n",
        "x1_min = min(x1)\n",
        "x1_max = max(x1)\n",
        "x2_min = min(x2)\n",
        "x2_max = max(x2)\n",
        "x3_min = min(x3)\n",
        "x3_max = max(x3)\n",
        "x4_min = min(x4)\n",
        "x4_max = max(x4)\n",
        "x5_min = min(x5)\n",
        "x5_max = max(x5)\n",
        "x1_test_min = min(x1_test)\n",
        "x1_test_max = max(x1_test)\n",
        "x2_test_min = min(x2_test)\n",
        "x2_test_max = max(x2_test)\n",
        "x3_test_min = min(x3_test)\n",
        "x3_test_max = max(x3_test)\n",
        "x4_test_min = min(x4_test)\n",
        "x4_test_max = max(x4_test)\n",
        "x5_test_min = min(x5_test)\n",
        "x5_test_max = max(x5_test)\n",
        "\n",
        "# Feature Scaling\n",
        "def feature_scaling(x, x_min, x_max):\n",
        "    return (x - x_min) / (x_max - x_min)\n",
        "\n",
        "x1_train = feature_scaling(x1, x1_min, x1_max)\n",
        "x2_train = feature_scaling(x2, x2_min, x2_max)\n",
        "x3_train = feature_scaling(x3, x3_min, x3_max)\n",
        "x4_train = feature_scaling(x4, x4_min, x4_max)\n",
        "x5_train = feature_scaling(x5, x5_min, x5_max)\n",
        "x1_test = feature_scaling(x1_test, x1_test_min, x1_test_max)\n",
        "x2_test = feature_scaling(x2_test, x2_test_min, x2_test_max)\n",
        "x3_test = feature_scaling(x3_test, x3_test_min, x3_test_max)\n",
        "x4_test = feature_scaling(x4_test, x4_test_min, x4_test_max)\n",
        "x5_test = feature_scaling(x5_test, x5_test_min, x5_test_max)"
      ],
      "metadata": {
        "id": "eoYAe4QbUzqG"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training model on data and getting results\n",
        "model = LogisticRegression()\n",
        "model.fit(\n",
        "    np.array([x1_train, x2_train, x3_train, x4_train, x5_train]).T,\n",
        "    y_train\n",
        ")\n",
        "\n",
        "# Predicting probabilities (not classes)\n",
        "y_pred_prob = model.predict_proba(\n",
        "    np.array([x1_train, x2_train, x3_train, x4_train, x5_train]).T\n",
        ")[:, 1]  # probabilities of class 1 (CHD = 1)\n",
        "\n",
        "# Custom threshold\n",
        "threshold = 0.2\n",
        "\n",
        "# Applying on training data\n",
        "y_pred = (y_pred_prob >= threshold).astype(int)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"Threshold:\", threshold)\n",
        "print(\"Accuracy:\", accuracy_score(y_train, y_pred))\n",
        "print(confusion_matrix(y_train, y_pred))\n",
        "print(classification_report(y_train, y_pred))\n",
        "\n",
        "# Predict probabilities on test data\n",
        "y_test_pred_prob = model.predict_proba(\n",
        "    np.array([x1_test, x2_test, x3_test, x4_test, x5_test]).T\n",
        ")[:, 1]\n",
        "\n",
        "# Applying on test data\n",
        "y_test_pred = (y_test_pred_prob >= threshold).astype(int)\n",
        "\n",
        "# Evaluate performance for test data\n",
        "print(\"Threshold:\", threshold)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "print(confusion_matrix(y_test, y_test_pred))\n",
        "print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sst6N2MtV71H",
        "outputId": "e9c146d6-c840-4df5-c98b-01deb4441a24"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold: 0.2\n",
            "Accuracy: 0.746922024623803\n",
            "[[1967  522]\n",
            " [ 218  217]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.79      0.84      2489\n",
            "           1       0.29      0.50      0.37       435\n",
            "\n",
            "    accuracy                           0.75      2924\n",
            "   macro avg       0.60      0.64      0.61      2924\n",
            "weighted avg       0.81      0.75      0.77      2924\n",
            "\n",
            "Threshold: 0.2\n",
            "Test Accuracy: 0.726775956284153\n",
            "[[481 129]\n",
            " [ 71  51]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.79      0.83       610\n",
            "           1       0.28      0.42      0.34       122\n",
            "\n",
            "    accuracy                           0.73       732\n",
            "   macro avg       0.58      0.60      0.58       732\n",
            "weighted avg       0.77      0.73      0.75       732\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing custom Logistic Regression code with regularization\n",
        "class LogisticRegressionCustom:\n",
        "    def __init__(self, x1, x2, x3, x4, x5, y):\n",
        "        self.x1 = x1\n",
        "        self.x2 = x2\n",
        "        self.x3 = x3\n",
        "        self.x4 = x4\n",
        "        self.x5 = x5\n",
        "        self.y = y\n",
        "        self.m = len(x1)\n",
        "\n",
        "    def calculate_hypothesis(\n",
        "        self, weight1, weight2, weight3, weight4, weight5,\n",
        "        bias, x1_data, x2_data, x3_data, x4_data, x5_data\n",
        "        ):\n",
        "\n",
        "        hypothesis = []\n",
        "\n",
        "        for i in range(len(x1_data)):\n",
        "          z = weight1 * x1_data[i] + weight2 * x2_data[i] + weight3 * x3_data[i] + weight4 * x4_data[i] + weight5 * x5_data[i] + bias\n",
        "          hypothesis.append(1 / (1 + np.exp(-z)))\n",
        "        return np.array(hypothesis) # Convert the list to a NumPy array\n",
        "\n",
        "    def calculate_cost(\n",
        "        self, weight1, weight2, weight3, weight4, weight5,\n",
        "        bias, lambda_\n",
        "        ):\n",
        "\n",
        "        hypothesis = self.calculate_hypothesis(\n",
        "                          weight1, weight2, weight3, weight4, weight5,\n",
        "                          bias, self.x1, self.x2, self.x3, self.x4, self.x5\n",
        "                          )\n",
        "\n",
        "        loss = -self.y * np.log(hypothesis) - (1 - self.y) * np.log(1 - hypothesis)\n",
        "        cost = np.sum(loss) / self.m\n",
        "        regularization_term = (lambda_ / (2 * self.m)) * (weight1 ** 2 + weight2 ** 2 + weight3 ** 2 + weight4 ** 2 + weight5 ** 2)\n",
        "        cost += regularization_term\n",
        "        return cost\n",
        "\n",
        "    def gradient_descent(self, learning_rate, weight1, weight2, weight3, weight4, weight5, bias, lambda_):\n",
        "        hypothesis = self.calculate_hypothesis(\n",
        "                          weight1, weight2, weight3, weight4, weight5,\n",
        "                          bias, self.x1, self.x2, self.x3, self.x4, self.x5\n",
        "                          )\n",
        "\n",
        "        weight1_gradient = 0\n",
        "        weight2_gradient = 0\n",
        "        weight3_gradient = 0\n",
        "        weight4_gradient = 0\n",
        "        weight5_gradient = 0\n",
        "        bias_gradient = 0\n",
        "\n",
        "        for i in range(self.m):\n",
        "            weight1_gradient += (hypothesis[i] - self.y[i]) * self.x1[i]\n",
        "            weight2_gradient += (hypothesis[i] - self.y[i]) * self.x2[i]\n",
        "            weight3_gradient += (hypothesis[i] - self.y[i]) * self.x3[i]\n",
        "            weight4_gradient += (hypothesis[i] - self.y[i]) * self.x4[i]\n",
        "            weight5_gradient += (hypothesis[i] - self.y[i]) * self.x5[i]\n",
        "            bias_gradient += (hypothesis[i] - self.y[i])\n",
        "\n",
        "        # Apply averaging and regularization correctly\n",
        "        weight1_gradient = (weight1_gradient / self.m) + (lambda_ / self.m) * weight1\n",
        "        weight2_gradient = (weight2_gradient / self.m) + (lambda_ / self.m) * weight2\n",
        "        weight3_gradient = (weight3_gradient / self.m) + (lambda_ / self.m) * weight3\n",
        "        weight4_gradient = (weight4_gradient / self.m) + (lambda_ / self.m) * weight4\n",
        "        weight5_gradient = (weight5_gradient / self.m) + (lambda_ / self.m) * weight5\n",
        "        bias_gradient = bias_gradient / self.m\n",
        "\n",
        "        # Update weights\n",
        "        weight1 -= learning_rate * weight1_gradient\n",
        "        weight2 -= learning_rate * weight2_gradient\n",
        "        weight3 -= learning_rate * weight3_gradient\n",
        "        weight4 -= learning_rate * weight4_gradient\n",
        "        weight5 -= learning_rate * weight5_gradient\n",
        "        bias -= learning_rate * bias_gradient\n",
        "\n",
        "        return weight1, weight2, weight3, weight4, weight5, bias\n",
        "\n",
        "    def train(self, learning_rate, weight1, weight2, weight3, weight4, weight5, bias, lambda_, epochs):\n",
        "        cost_history = []\n",
        "        weight1_history = []\n",
        "        weight2_history = []\n",
        "        weight3_history = []\n",
        "        weight4_history = []\n",
        "        weight5_history = []\n",
        "        bias_history = []\n",
        "\n",
        "        for i in range(epochs):\n",
        "            weight1, weight2, weight3, weight4, weight5, bias = self.gradient_descent(\n",
        "                                                                    learning_rate, weight1, weight2, weight3, weight4, weight5,\n",
        "                                                                    bias, lambda_\n",
        "                                                                    )\n",
        "\n",
        "            cost = self.calculate_cost(weight1, weight2, weight3, weight4, weight5, bias, lambda_)\n",
        "\n",
        "            weight1_history.append(weight1)\n",
        "            weight2_history.append(weight2)\n",
        "            weight3_history.append(weight3)\n",
        "            weight4_history.append(weight4)\n",
        "            weight5_history.append(weight5)\n",
        "            bias_history.append(bias)\n",
        "            cost_history.append(cost)\n",
        "\n",
        "        return cost_history, weight1_history, weight2_history, weight3_history, weight4_history, weight5_history, bias_history\n"
      ],
      "metadata": {
        "id": "ggB8E6Hecvkm"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling my own Logistic Regression Function for computation\n",
        "model = LogisticRegressionCustom(x1_train, x2_train, x3_train, x4_train, x5_train, y_train)\n",
        "\n",
        "# Call the train method and unpack its return values\n",
        "cost_history, weight1_history, weight2_history, weight3_history, weight4_history, weight5_history, bias_history = model.train(\n",
        "    learning_rate=0.01, weight1=1, weight2=1, weight3=1, weight4=1, weight5=1, bias=1, lambda_=0.9, epochs=1000\n",
        ")\n",
        "\n",
        "# Extrating final values to pass on predictions\n",
        "w1 = weight1_history[-1]\n",
        "w2 = weight2_history[-1]\n",
        "w3 = weight3_history[-1]\n",
        "w4 = weight4_history[-1]\n",
        "w5 = weight5_history[-1]\n",
        "b = bias_history[-1]\n",
        "\n",
        "# Checking for Probabilities\n",
        "def predict(weight1, weight2, weight3, weight4, weight5, bias, threshold):\n",
        "    hypothesis = model.calculate_hypothesis(\n",
        "                      weight1, weight2, weight3, weight4, weight5,\n",
        "                      bias, x1_train, x2_train, x3_train, x4_train, x5_train\n",
        "                      )\n",
        "\n",
        "    y_pred = (hypothesis >= threshold).astype(int)\n",
        "    return y_pred\n",
        "\n",
        "threshold = 0.2\n",
        "y_pred = predict(w1, w2, w3, w4, w5, b, threshold)\n",
        "\n",
        "# Evaluate performance for training data\n",
        "print(\"Threshold:\", threshold)\n",
        "print(\"Accuracy:\", accuracy_score(y_train, y_pred))\n",
        "print(confusion_matrix(y_train, y_pred))\n",
        "print(classification_report(y_train, y_pred))\n",
        "\n",
        "# Applying on test data\n",
        "y_test_pred_prob = model.calculate_hypothesis(\n",
        "                      w1, w2, w3, w4, w5,\n",
        "                      b, x1_test, x2_test, x3_test, x4_test, x5_test\n",
        "                      )\n",
        "y_test_pred = (y_test_pred_prob >= threshold).astype(int)\n",
        "\n",
        "# Evaluate performance for test data\n",
        "print(\"Threshold:\", threshold)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "print(confusion_matrix(y_test, y_test_pred))\n",
        "print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlscKg1plHmM",
        "outputId": "e394f82f-3c63-4dfb-963f-f5ba34c4410a"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold: 0.2\n",
            "Accuracy: 0.622093023255814\n",
            "[[1601  888]\n",
            " [ 217  218]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.64      0.74      2489\n",
            "           1       0.20      0.50      0.28       435\n",
            "\n",
            "    accuracy                           0.62      2924\n",
            "   macro avg       0.54      0.57      0.51      2924\n",
            "weighted avg       0.78      0.62      0.67      2924\n",
            "\n",
            "Threshold: 0.2\n",
            "Test Accuracy: 0.5819672131147541\n",
            "[[366 244]\n",
            " [ 62  60]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.60      0.71       610\n",
            "           1       0.20      0.49      0.28       122\n",
            "\n",
            "    accuracy                           0.58       732\n",
            "   macro avg       0.53      0.55      0.49       732\n",
            "weighted avg       0.75      0.58      0.63       732\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization avoided larger weight values to cause overfitting on the training data."
      ],
      "metadata": {
        "id": "fDaJ5JrO98xL"
      }
    }
  ]
}