{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UzyhBVkWsKHi",
        "outputId": "7d6e476b-931c-47ff-811a-073404ce8f51"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.13.9' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/irfaf/AppData/Local/Microsoft/WindowsApps/python3.13.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# ASSIGNMENT #02: Implementation and Comparison of Classification Algorithms\n",
        "# Course: CS-471 Machine Learning (BEE-14)\n",
        "# Student: Irfa Farooq\n",
        "# ==============================================================\n",
        "\n",
        "# Importing important libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        ")\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Part 1: Dataset Selection and Preprocessing\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['species'] = iris.target_names[y]\n",
        "\n",
        "# Print first 5 rows for overview of dataset\n",
        "print(\"Dataset sample:\\n\", df.head())\n",
        "\n",
        "# Select only two classes for binary classification: Setosa vs Versicolor\n",
        "df_binary = df[df['species'].isin(['setosa', 'versicolor'])]\n",
        "X = df_binary[['petal length (cm)', 'petal width (cm)']].values\n",
        "y = df_binary['species'].map({'setosa': 0, 'versicolor': 1}).values\n",
        "\n",
        "# Explain preprocessing choices in comments\n",
        "print(\"\\nDataset only contains 'setosa' and 'versicolor' classes.\")\n",
        "print(\"Using only 'petal length' and 'petal width' features for clear separation.\")\n",
        "# These two features provide nearly perfect linear separability between classes\n",
        "\n",
        "# Train-test split (80-20) with stratification to preserve class ratios\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize features for gradient-based models like Logistic Regression and SVM\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Part 2: From-Scratch Implementations\n",
        "\n",
        "# ---------------- Logistic Regression ----------------\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def logistic_train(X, y, lr=0.01, epochs=1000):\n",
        "    m, n = X.shape\n",
        "    w = np.zeros(n)  # Initialize weights\n",
        "    b = 0            # Initialize bias\n",
        "    for _ in range(epochs):\n",
        "        z = np.dot(X, w) + b\n",
        "        y_pred = sigmoid(z)\n",
        "        dw = (1/m) * np.dot(X.T, (y_pred - y))  # Gradient of weights\n",
        "        db = (1/m) * np.sum(y_pred - y)         # Gradient of bias\n",
        "        w -= lr * dw\n",
        "        b -= lr * db\n",
        "    return w, b\n",
        "\n",
        "def logistic_predict(X, w, b):\n",
        "    y_pred = sigmoid(np.dot(X, w) + b)\n",
        "    return (y_pred >= 0.5).astype(int)  # Threshold at 0.5\n",
        "\n",
        "# ---------------- Support Vector Machine ----------------\n",
        "def svm_train(X, y, lr=0.01, epochs=1000, C=1.0):\n",
        "    y_svm = np.where(y == 0, -1, 1)  # Convert {0,1} to {-1,+1}\n",
        "    m, n = X.shape\n",
        "    w = np.zeros(n)\n",
        "    b = 0\n",
        "    for _ in range(epochs):\n",
        "        for i in range(m):\n",
        "            condition = y_svm[i] * (np.dot(X[i], w) + b) >= 1\n",
        "            if condition:\n",
        "                # Only regularization term applied if margin satisfied\n",
        "                w -= lr * (2 * (1/epochs) * w)\n",
        "            else:\n",
        "                # Hinge loss gradient update + regularization\n",
        "                w -= lr * (2 * (1/epochs) * w - np.dot(X[i], y_svm[i]) * C)\n",
        "                b += lr * y_svm[i] * C\n",
        "    return w, b\n",
        "\n",
        "def svm_predict(X, w, b):\n",
        "    return np.sign(np.dot(X, w) + b)\n",
        "\n",
        "# ---------------- Gaussian Naive Bayes ----------------\n",
        "class GaussianNB:\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        self.mean = {}\n",
        "        self.var = {}\n",
        "        self.priors = {}\n",
        "        for c in self.classes:\n",
        "            X_c = X[y == c]\n",
        "            self.mean[c] = X_c.mean(axis=0)      # Mean per class\n",
        "            self.var[c] = X_c.var(axis=0)        # Variance per class\n",
        "            self.priors[c] = X_c.shape[0] / X.shape[0]  # Prior probability\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict(x) for x in X])\n",
        "\n",
        "    def _predict(self, x):\n",
        "        posteriors = []\n",
        "        for c in self.classes:\n",
        "            prior = np.log(self.priors[c])\n",
        "            likelihood = -0.5 * np.sum(np.log(2 * np.pi * self.var[c]))\n",
        "            likelihood -= 0.5 * np.sum(((x - self.mean[c]) ** 2) / self.var[c])\n",
        "            posteriors.append(prior + likelihood)\n",
        "        return self.classes[np.argmax(posteriors)]\n",
        "\n",
        "# Part 3: Model Training\n",
        "\n",
        "# Train from-scratch models\n",
        "w_log, b_log = logistic_train(X_train, y_train)\n",
        "y_pred_log = logistic_predict(X_test, w_log, b_log)\n",
        "\n",
        "w_svm, b_svm = svm_train(X_train, y_train)\n",
        "y_pred_svm = np.where(svm_predict(X_test, w_svm, b_svm) == 1, 1, 0)\n",
        "\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "y_pred_nb = nb.predict(X_test)\n",
        "\n",
        "# Train built-in classifiers\n",
        "dt = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "rf = RandomForestClassifier().fit(X_train, y_train)\n",
        "ada = AdaBoostClassifier().fit(X_train, y_train)\n",
        "\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "y_pred_ada = ada.predict(X_test)\n",
        "\n",
        "# Part 4: Evaluation\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': y_pred_log,\n",
        "    'SVM': y_pred_svm,\n",
        "    'Naive Bayes': y_pred_nb,\n",
        "    'Decision Tree': y_pred_dt,\n",
        "    'Random Forest': y_pred_rf,\n",
        "    'AdaBoost': y_pred_ada\n",
        "}\n",
        "\n",
        "results = []\n",
        "for name, y_pred in models.items():\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred)\n",
        "    rec = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    results.append([name, acc, prec, rec, f1])\n",
        "    # Print evaluation metrics\n",
        "    print(f\"{name}: Acc={acc:.2f}, Prec={prec:.2f}, Rec={rec:.2f}, F1={f1:.2f}\")\n",
        "\n",
        "results_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
        "print(\"\\nModel Comparison:\\n\", results_df)\n",
        "\n",
        "# Comment on evaluation:\n",
        "# Using only 'petal length' and 'petal width', classes are linearly separable\n",
        "# Hence all models achieve perfect scores: Accuracy=1.0, Precision=1.0, Recall=1.0, F1=1.0\n",
        "# No false positives or false negatives exist in predictions\n",
        "\n",
        "# Part 5: Visualization\n",
        "\n",
        "# --- Confusion Matrices ---\n",
        "for name, y_pred in models.items():\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'{name} - Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "# --- Accuracy Comparison Bar Plot ---\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(x='Model', y='Accuracy', data=results_df)\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# --- 2D Decision Boundary Visualizations ---\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
        "                     np.linspace(y_min, y_max, 300))\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "grid_scaled = scaler.transform(grid)\n",
        "\n",
        "# Predict on grid for visualization\n",
        "zz_log = (sigmoid(np.dot(grid_scaled, w_log) + b_log) >= 0.5).astype(int).reshape(xx.shape)\n",
        "zz_svm = np.sign(np.dot(grid_scaled, w_svm) + b_svm).reshape(xx.shape)\n",
        "zz_nb = nb.predict(grid_scaled).reshape(xx.shape)\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# --- Logistic Regression ---\n",
        "cs = axs[0].contourf(xx, yy, zz_log, alpha=0.3, cmap='coolwarm')\n",
        "scatter = axs[0].scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
        "axs[0].set_title('Logistic Regression')\n",
        "axs[0].set_xlabel('Petal Length (cm)')\n",
        "axs[0].set_ylabel('Petal Width (cm)')\n",
        "# Legend: fix for matplotlib warning\n",
        "handles = [plt.Line2D([], [], marker='o', color='w', markerfacecolor='blue', markersize=8),\n",
        "           plt.Line2D([], [], marker='o', color='w', markerfacecolor='red', markersize=8)]\n",
        "axs[0].legend(handles, ['Setosa', 'Versicolor'], loc='upper right')\n",
        "\n",
        "# --- SVM with decision boundary + margins ---\n",
        "axs[1].contourf(xx, yy, zz_svm, alpha=0.3, cmap='coolwarm')\n",
        "axs[1].scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
        "x_vals = np.linspace(x_min, x_max, 200)\n",
        "axs[1].set_title('Support Vector Machine')\n",
        "axs[1].set_xlabel('Petal Length (cm)')\n",
        "axs[1].set_ylabel('Petal Width (cm)')\n",
        "axs[1].legend(handles, ['Setosa', 'Versicolor'], loc='upper right')\n",
        "\n",
        "# --- Naive Bayes ---\n",
        "axs[2].contourf(xx, yy, zz_nb, alpha=0.3, cmap='coolwarm')\n",
        "scatter_nb = axs[2].scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
        "axs[2].set_title('Naive Bayes')\n",
        "axs[2].set_xlabel('Petal Length (cm)')\n",
        "axs[2].set_ylabel('Petal Width (cm)')\n",
        "axs[2].legend(handles, ['Setosa', 'Versicolor'], loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
